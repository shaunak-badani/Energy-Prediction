{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* This notebook serves the purpose of taking data from npz files and \n",
    "converting it into h5 format, so that in can be loaded dynamically \n",
    "into local memory while running the training model\n",
    "\n",
    "* It also saves the configuration to be used further into the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torchani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'name' : \"Uracil\",\n",
    "    'batches' : [35000, 70000, 105000],\n",
    "    'testing' : [105000, -1],\n",
    "    'share_path' : '/share1/shaunak/ML4NS/',\n",
    "    'dataset_name' : 'uracil_dft.npz',\n",
    "    'scratch_path' : '/scratch/shaunak/ML4NS',\n",
    "    'test_h5_filename' : 'testing.h5'\n",
    "}\n",
    "\n",
    "def save_config():\n",
    "    f = open('model_config', 'ab')\n",
    "    pickle.dump(config, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '{}/{}'.format(config['scratch_path'], config['name'])\n",
    "os.system('mkdir -p {}'.format(model_dir))\n",
    "\n",
    "from_path = '{}/{}/{}'.format(config['share_path'], \"datasets\", config['dataset_name'])\n",
    "os.system('rsync -aPs ada:{} {}'.format(from_path, model_dir))\n",
    "\n",
    "training_h5_path = \"{}/{}.h5\".format(model_dir, config['name'])\n",
    "testing_h5_path = \"{}/testing.h5\".format(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To reset if want to run notebook again, do not do it if you don't wish \n",
    "# to delete already generated h5 file\n",
    "os.system('rm {}'.format(training_h5_path))\n",
    "os.system('rm {}'.format(testing_h5_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    path = os.path.dirname(os.path.realpath(__file__))\n",
    "except NameError:\n",
    "    path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"{}/{}\".format(model_dir, config['dataset_name'])\n",
    "new_data_file = training_h5_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches , [35000, 70000, 105000]\n"
     ]
    }
   ],
   "source": [
    "molecule = np.load(data_path)\n",
    "molecule_name = config['name']\n",
    "batches = config['batches']\n",
    "names = [\"01\", \"02\", \"03\"]\n",
    "\n",
    "print(\"Batches ,\", batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Uracil', 'batches': [35000, 70000, 105000], 'testing': [105000, 133770], 'share_path': '/share1/shaunak/ML4NS/', 'dataset_name': 'uracil_dft.npz', 'scratch_path': '/scratch/shaunak/ML4NS', 'test_h5_filename': 'testing.h5'}\n"
     ]
    }
   ],
   "source": [
    "n = molecule['E'].shape[0]\n",
    "config['testing'][-1] = n\n",
    "os.system('rm model_config')\n",
    "save_config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_map = {\n",
    "    6 : \"C\".encode(\"utf-8\"),\n",
    "    1 : \"H\".encode(\"utf-8\"),\n",
    "    8 : \"O\".encode(\"utf-8\"),\n",
    "    7 : \"N\".encode(\"utf-8\"),\n",
    "}\n",
    "mult = 627.5094738898777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = list(map(lambda x:species_map[x], molecule['z']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species :  [b'C', b'C', b'N', b'C', b'N', b'C', b'O', b'O', b'H', b'H', b'H', b'H']\n"
     ]
    }
   ],
   "source": [
    "print(\"Species : \", species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file = h5py.File(new_data_file, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if molecule_name not in h5_file:\n",
    "    b = h5_file.create_group(molecule_name)\n",
    "else:\n",
    "    b = h5_file[molecule_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 35000\n",
      "35000 70000\n",
      "70000 105000\n"
     ]
    }
   ],
   "source": [
    "init = 0\n",
    "for i in range(len(batches)):\n",
    "    if names[i] not in b:\n",
    "        sub_group = b.create_group(names[i])\n",
    "    else:\n",
    "        sub_group = b[names[i]]\n",
    "    last = batches[i]\n",
    "    print(init, last)\n",
    "    if \"coordinates\" not in sub_group:\n",
    "        sub_group.create_dataset(\"coordinates\", data = molecule['R'][init:last])\n",
    "    if \"energies\" not in sub_group:\n",
    "        sub_group.create_dataset(\"energies\", data = molecule['E'][init:last].flatten() / mult) \n",
    "    if \"species\" not in sub_group:\n",
    "        sub_group.create_dataset(\"species\", data = species)\n",
    "    init = last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing to h5 file : \n",
      "<HDF5 dataset \"coordinates\": shape (35000, 12, 3), type \"<f8\">\n",
      "<HDF5 dataset \"energies\": shape (35000,), type \"<f8\">\n",
      "<HDF5 dataset \"species\": shape (12,), type \"|S1\">\n",
      " \n",
      "<HDF5 dataset \"coordinates\": shape (35000, 12, 3), type \"<f8\">\n",
      "<HDF5 dataset \"energies\": shape (35000,), type \"<f8\">\n",
      "<HDF5 dataset \"species\": shape (12,), type \"|S1\">\n",
      " \n",
      "<HDF5 dataset \"coordinates\": shape (35000, 12, 3), type \"<f8\">\n",
      "<HDF5 dataset \"energies\": shape (35000,), type \"<f8\">\n",
      "<HDF5 dataset \"species\": shape (12,), type \"|S1\">\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"Done with writing to h5 file : \")\n",
    "for i in names:\n",
    "    print(h5_file[molecule_name][i]['coordinates'])\n",
    "    print(h5_file[molecule_name][i]['energies'])\n",
    "    print(h5_file[molecule_name][i]['species'])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing h5 file ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Closing h5 file ...\")\n",
    "h5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "init, last = config['testing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105000 133770\n"
     ]
    }
   ],
   "source": [
    "print(init, last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_file = os.path.join(path, 'tmp_testing.h5')\n",
    "# test_h5_file = h5py.File(test_data_file, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file = testing_h5_path\n",
    "os.system(\"rm {}\".format(test_data_file))\n",
    "h5_file = h5py.File(test_data_file, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if molecule_name not in h5_file:\n",
    "    b = h5_file.create_group(molecule_name)\n",
    "else:\n",
    "    b = h5_file[molecule_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name not in b:\n",
    "    sub_group = b.create_group(name)\n",
    "else:\n",
    "    sub_group = b[name]\n",
    "\n",
    "if \"coordinates\" not in sub_group:\n",
    "    sub_group.create_dataset(\"coordinates\", data = molecule['R'][init:last])\n",
    "if \"energies\" not in sub_group:\n",
    "    sub_group.create_dataset(\"energies\", data = molecule['E'][init:last].flatten() / mult) \n",
    "if \"species\" not in sub_group:\n",
    "    sub_group.create_dataset(\"species\", data = species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing to temporary h5 file : \n"
     ]
    }
   ],
   "source": [
    "print(\"Done with writing to temporary h5 file : \")\n",
    "h5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferring generated files back to share1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Transferring generated files back to share1\")\n",
    "\n",
    "share_dir = \"{}/{}\".format(config['share_path'], config['name'])\n",
    "os.system('rsync -aPs --rsync-path=\"mkdir -p {} && rsync\"  {} {} ada:{}'.\\\n",
    "          format(share_dir, training_h5_path, testing_h5_path, share_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
